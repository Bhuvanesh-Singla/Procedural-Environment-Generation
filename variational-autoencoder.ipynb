{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7487917,"sourceType":"datasetVersion","datasetId":4290955}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport glob\nfilenames=glob.glob(os.path.join(\"/kaggle/input/procedural-environment-generation/dataset/dataset/\"+\"*.png\"))\nprint(len(filenames))\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch-summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\nfrom PIL import Image as I\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as transforms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef reparametrize(mu, logvar):\n    std = logvar.div(2).exp()\n    eps = Variable(std.data.new(std.size()).normal_())\n    return mu + std*eps\n\n\nclass View(nn.Module):\n    def __init__(self, size):\n        super(View, self).__init__()\n        self.size = size\n\n    def forward(self, tensor):\n        return tensor.view(self.size)\n\n\nclass BetaVAE_H(nn.Module):\n    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017).\"\"\"\n\n    def __init__(self, z_dim=10, nc=3):\n        super(BetaVAE_H, self).__init__()\n        self.z_dim = z_dim\n        self.nc = nc\n        self.encoder = nn.Sequential(\n            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n            nn.ReLU(True),\n            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n            nn.ReLU(True),\n            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n            nn.ReLU(True),\n            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n            nn.ReLU(True),\n            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n            nn.ReLU(True),\n            View((-1, 256*1*1)),                 # B, 256\n            nn.Linear(256, z_dim*2),             # B, z_dim*2\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(z_dim, 256),               # B, 256\n            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n        )\n\n        self.weight_init()\n\n    def weight_init(self):\n        for block in self._modules:\n            for m in self._modules[block]:\n                kaiming_init(m)\n\n    def forward(self, x):\n        distributions = self._encode(x)\n        mu = distributions[:, :self.z_dim]\n        logvar = distributions[:, self.z_dim:]\n        z = reparametrize(mu, logvar)\n        x_recon = self._decode(z)\n\n        return x_recon, mu, logvar\n\n    def _encode(self, x):\n        return self.encoder(x)\n\n    def _decode(self, z):\n        return self.decoder(z)\n\n\nclass BetaVAE_B(BetaVAE_H):\n    \"\"\"Model proposed in understanding beta-VAE paper(Burgess et al, arxiv:1804.03599, 2018).\"\"\"\n\n    def __init__(self, z_dim=10, nc=1):\n        super(BetaVAE_B, self).__init__()\n        self.nc = nc\n        self.z_dim = z_dim\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n            nn.ReLU(True),\n            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n            nn.ReLU(True),\n            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32,  8,  8\n            nn.ReLU(True),\n            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32,  4,  4\n            nn.ReLU(True),\n            View((-1, 32*4*4)),                  # B, 512\n            nn.Linear(32*4*4, 256),              # B, 256\n            nn.ReLU(True),\n            nn.Linear(256, 256),                 # B, 256\n            nn.ReLU(True),\n            nn.Linear(256, z_dim*2),             # B, z_dim*2\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(z_dim, 256),               # B, 256\n            nn.ReLU(True),\n            nn.Linear(256, 256),                 # B, 256\n            nn.ReLU(True),\n            nn.Linear(256, 32*4*4),              # B, 512\n            nn.ReLU(True),\n            View((-1, 32, 4, 4)),                # B,  32,  4,  4\n            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32,  8,  8\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 16, 16\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, nc, 4, 2, 1), # B,  nc, 64, 64\n        )\n        self.weight_init()\n\n    def weight_init(self):\n        for block in self._modules:\n            for m in self._modules[block]:\n                kaiming_init(m)\n\n    def forward(self, x):\n        distributions = self._encode(x)\n        mu = distributions[:, :self.z_dim]\n        logvar = distributions[:, self.z_dim:]\n        z = reparametrize(mu, logvar)\n        x_recon = self._decode(z).view(x.size())\n\n        return x_recon, mu, logvar\n\n    def _encode(self, x):\n        return self.encoder(x)\n\n    def _decode(self, z):\n        return self.decoder(z)\n\n\ndef kaiming_init(m):\n    if isinstance(m, (nn.Linear, nn.Conv2d)):\n        init.kaiming_normal(m.weight)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n        m.weight.data.fill_(1)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n\n\ndef normal_init(m, mean, std):\n    if isinstance(m, (nn.Linear, nn.Conv2d)):\n        m.weight.data.normal_(mean, std)\n        if m.bias.data is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n        m.weight.data.fill_(1)\n        if m.bias.data is not None:\n            m.bias.data.zero_()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class VAE(nn.Module):\n#     def __init__(self):\n#         super(VAE, self).__init__()\n\n#         self.encoder = nn.Sequential(\n#             nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), #64,128 128\n#             nn.ReLU(),\n#             nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), #128, 64 64\n#             nn.ReLU(),\n#             nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), #256 32 32\n#             nn.ReLU(),\n#             nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), #256 16 16\n#             nn.ReLU(),\n#             nn.Flatten()\n#         )\n\n#         self.fc_mu = nn.Linear(512*17*17, 64)\n#         self.fc_logvar = nn.Linear(512 * 17 * 17, 64)\n\n#         self.decoder = nn.Sequential(\n#             nn.Linear(64,512 * 17 * 17),\n#             nn.ReLU(),\n#             nn.Unflatten(1, (512,17,17)),\n#             nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=1), #256 16 16\n#             nn.ReLU(),\n#             nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=1),  #128 32 32\n#             nn.ReLU(),\n#             nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=1),  #64 64 64\n#             nn.ReLU(),\n#             nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2, padding=1), #32 128 128\n            \n#             nn.Sigmoid()\n#         )\n\n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n\n#     def forward(self, x):\n#         siz=x.size()\n#         x = self.encoder(x)\n#         mu = self.fc_mu(x)\n#         logvar = self.fc_logvar(x)\n#         z = self.reparameterize(mu, logvar)\n# #         z = z.view(-1, 512, 1, 1)\n#         x_reconstructed = self.decoder(z).view([64,1,254,254])\n#         return x_reconstructed, mu, logvar\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=BetaVAE_B(64).to(device)\nsummary(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading images\ndef load_image(paths):\n    for path in paths:\n        img=np.array(I.open(path))\n        img_normal=(img-np.min(img))/(np.max(img)-np.min(img))\n        yield np.transpose(np.expand_dims(np.float32(img_normal), axis = 2),(2,0,1))\n        \nclass Terrains():\n    def __init__(self,paths):\n        self.paths=paths\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self,id=None):\n        if torch.is_tensor(id):\n            id=id.tolist()\n        image=next(load_image([self.paths[id]]))\n        return image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=64\ntrain_set=Terrains(filenames)\ndevice='cuda' if torch.cuda.is_available() == True else 'cpu'\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(5,5,figsize=(14,14))\nsample=[next(load_image(filenames)) for i in range(25)]\nimages=load_image(filenames)\nidx=0\nfor i in range(5):\n    for j in range(5):\n        ax[i,j].imshow(np.transpose(next(images), (1,2,0)), cmap = 'gray')\n        idx+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Batches = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CP_dir = 'CP_VAE'\nos.makedirs(CP_dir, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(load_image(filenames)).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 3e-4\nepochs=200\ncriterion =nn.MSELoss()#nn.BCELoss()#nn.MSELoss()\n# nn.CrossEntropyLoss()\nbeta1=0.9\nbeta2=0.999\noptimizer= optim.Adam(model.parameters(), lr=lr,betas=(beta1, beta2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train\n\nfor epoch in range(epochs):\n    for i, data in enumerate(Batches,0):\n#         print(data.size())\n        inp= data.to(device)\n        inputs = inp\n        \n        # Train VAE\n        optimizer.zero_grad()\n        reconstructions, mean, log_var = model(inputs)\n#         print(reconstructions, inputs)\n        \n        reconstruction_loss = criterion(reconstructions, inputs)\n        \n        kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n        loss = reconstruction_loss + kl_divergence\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 64 == 0:  # Adjust the interval based on your needs\n            print(f'Epoch [{epoch + 1}/{epochs}], Batch [{i + 1}/{len(Batches)}], Loss: {loss.item()}')\n    with torch.no_grad():\n        model.eval()\n        x = torch.randn(64, 1, 256, 256).to(device)\n        y, _, _ = model(x)\n        generate_image = y[0][0].cpu().detach()\n\n    generate_image_np = generate_image.numpy().squeeze()\n\n    generate_image_np = (generate_image_np * 255).clip(0, 255) / 255.0\n    image_name = f'generate_img_epoch_{epoch + 1}.png'\n    image_path = os.path.join(CP_dir, image_name)\n    plt.imshow(generate_image_np, cmap='gray')\n    plt.imsave(image_path, generate_image_np, cmap='gray')\n\n    plt.title(f'Generated Image - Epoch {epoch + 1}')\n    plt.show()\n\n    # model checkpoints\n    checkpoint_vae = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint_vae, os.path.join(CP_dir, f'vae_checkpoint_epoch_{epoch + 1}.pth'))\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(64,1, 256, 256).to(device)\ny, _, _ = model(x)\n# m = torch.nn.Upsample(scale_factor=2, mode='nearest')\nm=torch.nn.functional.interpolate\ny_=m(y,[256,256])\ngenerate_image = y[0][0].cpu().detach()\ngenerate_image_ = y_[0][0].cpu().detach()\ngenerate_image_np = generate_image.numpy().squeeze()\ngenerate_image_np_ = generate_image_.numpy().squeeze()\ngenerate_image_np = (generate_image_np * 255).clip(0, 255) / 255.0\ngenerate_image_np_ = (generate_image_np_ * 255).clip(0, 255) / 255.0\nplt.imshow(generate_image_np, cmap='gray')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(generate_image_np_, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}