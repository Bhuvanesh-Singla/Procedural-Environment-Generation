{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7487917,
          "sourceType": "datasetVersion",
          "datasetId": 4290955
        },
        {
          "sourceId": 7714558,
          "sourceType": "datasetVersion",
          "datasetId": 4505307
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "DCGAN V1",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'procedural-environment-generation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4290955%2F7487917%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T083559Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3b8ca034eac44b5335961f4f1568fbd687d4bd14fd8abd38aa0b5e5820e3fab834f834738dafc40d0dd710f3102caaf9ca42f400ac736e3e3541763564796c5427c80b11ae800cd6aee53656a817f15169048f70bd2a84a04128ce8a3a34e8d22db70da69f10d026d08bb3d07b14e062b4bd4d6fa9f9ff23581fe2f44d458e1e41e0386774479cd823c484785b2165d05ef75ea5d60bd7f96d745cf80d2350bea9a5dce67482e49de92aed01e7aabad2b50323310ae66d1226bf627cd8bd61d96204a68ad5621846db7fc21a11dba2ed22517b6f0815353c9c31a7456ce74d2da130f6030c6cfef70585b6b07d078e50c5b359b7acb560c31b7b5ccfca1b1fd3,checkpoint:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4505307%2F7714558%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T083559Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3480de777e12d0ca1bfed7363ab016fd18c35fd988c63b0a9f869b59f35c239b51f8422984e772d604e88733b14bf1e39f2909b508eca869450713c06ac0d861d7f9efeb32601ba48b57fa8d73053f6c97c88741d5b0c52db2119796f5028a038e1862074ea061079361f3378d1270ff60403b291036f256ed7b68ccd46d39b5d94b904841f69428ae29d7fb2232456eabfbd08f6d2300ed10247e22a4547458017d010fdc1159fef603b7bc53c33092e8e60acbb26c4f6e0eefc7b4c2456161df1f2f5f6d1a5e4ee68424fed1eeef9603fdea78571875114bad33a475e6381a28b8310ce6b078617d8669b2e4dc7f02ccd924c11047e9965f085a70b1cbf891'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ge1dZ5w2TZOq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBMYtkxMTZOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:39.784736Z",
          "iopub.execute_input": "2024-02-27T13:30:39.785285Z",
          "iopub.status.idle": "2024-02-27T13:30:45.079599Z",
          "shell.execute_reply.started": "2024-02-27T13:30:39.785239Z",
          "shell.execute_reply": "2024-02-27T13:30:45.07819Z"
        },
        "trusted": true,
        "id": "bIpNbaQWTZOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is a random noise vector of size nz\n",
        "            # Output: (ngf * 16) x 4 x 4 (initial layer is doubled for larger image)\n",
        "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output: (ngf * 8) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output: (ngf * 4) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output: (ngf * 2) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output: (ngf) x 64 x 64\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output: (nc) x 256 x 256\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 4, 0, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.082591Z",
          "iopub.execute_input": "2024-02-27T13:30:45.08374Z",
          "iopub.status.idle": "2024-02-27T13:30:45.096955Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.083671Z",
          "shell.execute_reply": "2024-02-27T13:30:45.09485Z"
        },
        "trusted": true,
        "id": "iV5REbXGTZOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 256 x 256\n",
        "            nn.Conv2d(nc, ndf, 4, 4, 0, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size: (ndf) x 64 x 64\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size: (ndf*2) x 32 x 32\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size: (ndf*4) x 16 x 16\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size: (ndf*8) x 8 x 8\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.098942Z",
          "iopub.execute_input": "2024-02-27T13:30:45.100283Z",
          "iopub.status.idle": "2024-02-27T13:30:45.120281Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.100222Z",
          "shell.execute_reply": "2024-02-27T13:30:45.119062Z"
        },
        "trusted": true,
        "id": "LbkpCURwTZOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_data = \"/kaggle/input/procedural-environment-generation/original_raw/original_raw\"\n",
        "img_shape = (256,256,1)\n",
        "img_paths = [dir_data + '/' + file for file in os.listdir(dir_data)]\n",
        "img_paths = img_paths[:-6]\n",
        "img_paths = np.sort(img_paths)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.122183Z",
          "iopub.execute_input": "2024-02-27T13:30:45.123329Z",
          "iopub.status.idle": "2024-02-27T13:30:45.569076Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.12327Z",
          "shell.execute_reply": "2024-02-27T13:30:45.566894Z"
        },
        "trusted": true,
        "id": "hZCH_vcITZOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(img_paths):\n",
        "    for image_path in img_paths:\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Check if the difference between max and min values is greater than threshold\n",
        "            if np.max(img_array) - np.min(img_array) < 50:\n",
        "                continue  # Skip this image if condition is not met\n",
        "\n",
        "            # Check for NaN values and replace them with zero\n",
        "            img_array = np.nan_to_num(img_array)\n",
        "\n",
        "            # Normalize the image array\n",
        "            min_val = np.min(img_array)\n",
        "            max_val = np.max(img_array)\n",
        "            if min_val != max_val:\n",
        "                img_array_normalized = (img_array - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                img_array_normalized = img_array\n",
        "\n",
        "            # Transpose and expand dimensions\n",
        "            img_array_normalized = np.transpose(np.expand_dims(np.float32(img_array_normalized), axis=2), (2, 0, 1))\n",
        "\n",
        "            yield img_array_normalized\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.573395Z",
          "iopub.execute_input": "2024-02-27T13:30:45.576031Z",
          "iopub.status.idle": "2024-02-27T13:30:45.584405Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.575957Z",
          "shell.execute_reply": "2024-02-27T13:30:45.582429Z"
        },
        "trusted": true,
        "id": "hTxXZj60TZOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(imgs, grid_size = 5):\n",
        "\n",
        "    fig = plt.figure(figsize = (9, 9))\n",
        "    plt.axis(\"off\")\n",
        "    columns = rows = grid_size\n",
        "    plt.title(\"Training Images\")\n",
        "    images = load_images(imgs)\n",
        "    for i in range(1, columns*rows +1):\n",
        "\n",
        "        fig.add_subplot(rows, columns, i)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(np.transpose(next(images), (1,2,0)), cmap = 'gray')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.586896Z",
          "iopub.execute_input": "2024-02-27T13:30:45.588066Z",
          "iopub.status.idle": "2024-02-27T13:30:45.598919Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.58799Z",
          "shell.execute_reply": "2024-02-27T13:30:45.596213Z"
        },
        "trusted": true,
        "id": "nLCCkropTZOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = 'cuda:0' if torch.cuda.is_available() == True else 'cpu'\n",
        "device = torch.device(dev)\n",
        "plot_images(img_paths, 5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:45.601431Z",
          "iopub.execute_input": "2024-02-27T13:30:45.601922Z",
          "iopub.status.idle": "2024-02-27T13:30:47.20236Z",
          "shell.execute_reply.started": "2024-02-27T13:30:45.601876Z",
          "shell.execute_reply": "2024-02-27T13:30:47.201252Z"
        },
        "trusted": true,
        "id": "4z8JNptkTZOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Terrains(Dataset):\n",
        "    def __init__(self, img_paths):\n",
        "        '''np array containing all paths to all images'''\n",
        "        self.img_paths = img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image = next(load_images([self.img_paths[idx]]))\n",
        "        return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:47.203784Z",
          "iopub.execute_input": "2024-02-27T13:30:47.204938Z",
          "iopub.status.idle": "2024-02-27T13:30:47.211672Z",
          "shell.execute_reply.started": "2024-02-27T13:30:47.204891Z",
          "shell.execute_reply": "2024-02-27T13:30:47.210451Z"
        },
        "trusted": true,
        "id": "XpNsrDJ1TZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = Terrains(img_paths)\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "dset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:47.213316Z",
          "iopub.execute_input": "2024-02-27T13:30:47.213704Z",
          "iopub.status.idle": "2024-02-27T13:30:47.233182Z",
          "shell.execute_reply.started": "2024-02-27T13:30:47.213672Z",
          "shell.execute_reply": "2024-02-27T13:30:47.231865Z"
        },
        "trusted": true,
        "id": "kR_WFp2qTZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset = dset, batch_size = batch_size, shuffle = shuffle)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:47.235139Z",
          "iopub.execute_input": "2024-02-27T13:30:47.236202Z",
          "iopub.status.idle": "2024-02-27T13:30:47.244139Z",
          "shell.execute_reply.started": "2024-02-27T13:30:47.23605Z",
          "shell.execute_reply": "2024-02-27T13:30:47.243088Z"
        },
        "trusted": true,
        "id": "SXSCq7QUTZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training_imgs\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:64], padding = 2, normalize=True).cpu(),(1,2,0)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:47.245576Z",
          "iopub.execute_input": "2024-02-27T13:30:47.246208Z",
          "iopub.status.idle": "2024-02-27T13:30:48.494187Z",
          "shell.execute_reply.started": "2024-02-27T13:30:47.246168Z",
          "shell.execute_reply": "2024-02-27T13:30:48.492811Z"
        },
        "trusted": true,
        "id": "QZRPpI1bTZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.496107Z",
          "iopub.execute_input": "2024-02-27T13:30:48.496532Z",
          "iopub.status.idle": "2024-02-27T13:30:48.504439Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.496496Z",
          "shell.execute_reply": "2024-02-27T13:30:48.502795Z"
        },
        "trusted": true,
        "id": "p9kamfavTZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "#\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 256\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.506705Z",
          "iopub.execute_input": "2024-02-27T13:30:48.507264Z",
          "iopub.status.idle": "2024-02-27T13:30:48.516015Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.507221Z",
          "shell.execute_reply": "2024-02-27T13:30:48.514688Z"
        },
        "trusted": true,
        "id": "YDXAdaoZTZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "D_lr = 3e-6\n",
        "G_lr = 3e-4\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.520038Z",
          "iopub.execute_input": "2024-02-27T13:30:48.520584Z",
          "iopub.status.idle": "2024-02-27T13:30:48.532173Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.520498Z",
          "shell.execute_reply": "2024-02-27T13:30:48.530672Z"
        },
        "trusted": true,
        "id": "-S30lQhYTZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nt9enf90TZO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "fixed_noise = torch.randn(batch_size, nz, 1,1,device = device)\n",
        "\n",
        "workers = 2\n",
        "num_epochs = 200\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "netG = Generator(ngpu).to(device)  # Move to GPU\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = D_lr, betas = (beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = G_lr, betas = (beta1, 0.999))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.533918Z",
          "iopub.execute_input": "2024-02-27T13:30:48.534373Z",
          "iopub.status.idle": "2024-02-27T13:30:48.860647Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.534333Z",
          "shell.execute_reply": "2024-02-27T13:30:48.859462Z"
        },
        "trusted": true,
        "id": "x2V_gzr6TZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.862307Z",
          "iopub.execute_input": "2024-02-27T13:30:48.862716Z",
          "iopub.status.idle": "2024-02-27T13:30:48.871684Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.862682Z",
          "shell.execute_reply": "2024-02-27T13:30:48.870043Z"
        },
        "trusted": true,
        "id": "4NhWvqwSTZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim=100\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.874004Z",
          "iopub.execute_input": "2024-02-27T13:30:48.874482Z",
          "iopub.status.idle": "2024-02-27T13:30:48.882104Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.874445Z",
          "shell.execute_reply": "2024-02-27T13:30:48.880893Z"
        },
        "trusted": true,
        "id": "H6z2aFFzTZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir=\"/kaggle/working/saved_checkpoints\"\n",
        "os.makedirs(checkpoint_dir,exist_ok=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.883954Z",
          "iopub.execute_input": "2024-02-27T13:30:48.884414Z",
          "iopub.status.idle": "2024-02-27T13:30:48.893682Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.884377Z",
          "shell.execute_reply": "2024-02-27T13:30:48.892084Z"
        },
        "trusted": true,
        "id": "SsaJUtBPTZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "import os\n",
        "# Function to save a checkpoint\n",
        "epoch=95\n",
        "print(\"Starting Training Loop...\")\n",
        "checkp_dir='/kaggle/input/checkpoint'\n",
        "\n",
        "checkpoint_netG = torch.load(os.path.join(checkp_dir, 'netG_checkpoint_epoch_{}.pth'.format(epoch)))\n",
        "netG.load_state_dict(checkpoint_netG['model_state_dict'])\n",
        "optimizerG.load_state_dict(checkpoint_netG['optimizer_state_dict'])\n",
        "start_epoch_G = checkpoint_netG['epoch'] + 1\n",
        "loss_G = checkpoint_netG['loss']\n",
        "\n",
        "# Load discriminator checkpoint\n",
        "checkpoint_netD = torch.load(os.path.join(checkp_dir, 'netD_checkpoint_epoch_{}.pth'.format(epoch)))\n",
        "netD.load_state_dict(checkpoint_netD['model_state_dict'])\n",
        "optimizerD.load_state_dict(checkpoint_netD['optimizer_state_dict'])\n",
        "start_epoch_D = checkpoint_netD['epoch'] + 1\n",
        "loss_D = checkpoint_netD['loss']\n",
        "# Function to load a checkpoint\n",
        "checkpoint_frequency = 5\n",
        "\n",
        "for epoch in range(start_epoch_D,num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "         ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        ##train with all reals\n",
        "        netD.zero_grad()\n",
        "        #setup batch\n",
        "        real_cpu = data.to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,1,5,5), real_label, dtype=torch.float, device = device)\n",
        "\n",
        "        #fwd pass thru D\n",
        "        output = netD(real_cpu)\n",
        "        #calc loss on real\n",
        "        errD_real = criterion(output, label)\n",
        "        #calc grads for D\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ##train with all fakes\n",
        "        #gen batch of latent\n",
        "        noise = torch.randn(b_size, nz,1,1, device = device)\n",
        "        #generate fake batch by G\n",
        "#         print(noise.shape)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        #classify all fakes with D\n",
        "        output = netD(fake.detach())\n",
        "        #calc D's loss\n",
        "        errD_fake = criterion(output, label)\n",
        "        #calc gradients accumulates with prev grads\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        #compute errD as sum of fake and real\n",
        "        errD = errD_real + errD_fake\n",
        "\n",
        "        #update D\n",
        "\n",
        "        optimizerD.step()\n",
        "\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)# fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "#         print(output)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i%50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        if (epoch%9==0) and (i%50 == 0):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            fig = plt.figure(figsize=(8,8))\n",
        "            plt.axis(\"off\")\n",
        "            ims = [[ plt.imshow(np.transpose(i,(1,2,0)), cmap = 'gray')]for i in img_list]\n",
        "            plt.show()\n",
        "\n",
        "            img_list = []\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "    if epoch % checkpoint_frequency == 0:\n",
        "        # Save generator checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': netG.state_dict(),\n",
        "            'optimizer_state_dict': optimizerG.state_dict(),\n",
        "            'loss': errG.item(),\n",
        "        }, os.path.join(checkpoint_dir, f'netG_checkpoint_epoch_{epoch}.pth'))\n",
        "\n",
        "        # Save discriminator checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': netD.state_dict(),\n",
        "            'optimizer_state_dict': optimizerD.state_dict(),\n",
        "            'loss': errD.item(),\n",
        "        }, os.path.join(checkpoint_dir, f'netD_checkpoint_epoch_{epoch}.pth'))\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:30:48.895381Z",
          "iopub.execute_input": "2024-02-27T13:30:48.895845Z",
          "iopub.status.idle": "2024-02-27T13:35:11.376237Z",
          "shell.execute_reply.started": "2024-02-27T13:30:48.895808Z",
          "shell.execute_reply": "2024-02-27T13:35:11.374727Z"
        },
        "trusted": true,
        "id": "gEBNP78OTZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T13:35:11.377461Z",
          "iopub.status.idle": "2024-02-27T13:35:11.378642Z",
          "shell.execute_reply.started": "2024-02-27T13:35:11.378365Z",
          "shell.execute_reply": "2024-02-27T13:35:11.378394Z"
        },
        "trusted": true,
        "id": "t0Uncw0oTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "LOiIMXQrTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eTTbHUXATZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ynJqZZ41TZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XrhifWmZTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "T5Bb14IDTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "h0Qb7LqPTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wzglWOFsTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "_88mMv4ETZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cpHDrPEETZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QIGzhJsrTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "RW0zY01WTZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "W9UHbkZ0TZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "aDwbFQY9TZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Oldg8VTfTZO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}