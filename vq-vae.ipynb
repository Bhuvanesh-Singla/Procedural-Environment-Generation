{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7487917,"sourceType":"datasetVersion","datasetId":4290955}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport glob\nfilenames=glob.glob(os.path.join(\"/kaggle/input/procedural-environment-generation/dataset/dataset/\"+\"*.png\"))\nprint(len(filenames))\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch-summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\nfrom PIL import Image as I\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading images\ndef load_image(paths):\n    for path in paths:\n        img=np.array(I.open(path))\n        img_normal=(img-np.min(img))/(np.max(img)-np.min(img))\n        yield np.transpose(np.expand_dims(np.float32(img_normal), axis = 2),(2,0,1))\n        \nclass Terrains():\n    def __init__(self,paths):\n        self.paths=paths\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self,id=None):\n        if torch.is_tensor(id):\n            id=id.tolist()\n        image=next(load_image([self.paths[id]]))\n        return image\nnext(load_image(filenames)).shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=45\n\ndevice='cuda' if torch.cuda.is_available() == True else 'cpu'\nif device=='cuda':\n    dev1='cuda:0'\n    dev2='cuda:1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(5,5,figsize=(14,14))\nsample=[next(load_image(filenames)) for i in range(25)]\nimages=load_image(filenames)\nidx=0\nfor i in range(5):\n    for j in range(5):\n        ax[i,j].imshow(np.transpose(next(images), (1,2,0)), cmap = 'gray')\n        idx+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set=Terrains(filenames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Batches = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True) #var=0.0572","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(Batches.dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean=0\nfor i in range(84422):\n    mean+=np.mean(Batches.dataset.__getitem__(i))\nprint(mean/84422)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var=0\nfor i in range(84422):\n    var+=np.var(Batches.dataset.__getitem__(i))\nprint(var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_dat(data_loader):\n    total_samples = 0\n    mean = 0.\n    mean_sq = 0.\n    batch_mean=0.\n    # Iterate over batches\n    for batch in data_loader:\n        images, _ = batch  # Assuming the batch is a tuple of (images, labels)\n        batch_size = images.size(0)\n        # print(images.size())\n        total_samples += batch_size\n\n        # Compute mean and mean of squares for the current batch\n        # for i in range(batch_size):\n        batch_mean = torch.mean(images, dim=(0, 2,3))  # Mean along batch, height, and width dimensions\n\n        # Update overall mean and mean of squares\n        mean += batch_mean* batch_size\n        # print(mean)\n\n    # Finalize mean and mean of squares\n    mean /= total_samples\n    return mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CP_dir = 'CP_VQ_VAE'\nos.makedirs(CP_dir, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass GroupNorm(nn.Module):\n    def __init__(self, in_channels):\n        super(GroupNorm, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n    def forward(self, x):\n        return self.gn(x)\n\nclass NonLocalBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = GroupNorm(in_channels)\n        self.q = torch.nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n        self.k = torch.nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n        self.v = torch.nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n        self.proj_out = torch.nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n\n    def forward(self, x):\n        h_ = self.norm(x)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        b, c, h, w = q.shape\n\n        q = q.reshape(b, c, h * w)\n        q = q.permute(0, 2, 1)\n        k = k.reshape(b, c, h * w)\n        v = v.reshape(b, c, h * w)\n\n        attn = torch.bmm(q, k)\n        attn = attn * (int(c) ** (-0.5))\n        attn = F.softmax(attn, dim=2)\n\n        attn = attn.permute(0, 2, 1)\n        A = torch.bmm(v, attn)\n        A = A.reshape(b, c, h, w)\n\n        A = self.proj_out(A)\n\n        return x + A\nclass UpSampleBlock(nn.Module):\n    def __init__(self, channels):\n        super(UpSampleBlock, self).__init__()\n        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.)\n        return self.conv(x)\nclass DownSampleBlock(nn.Module):\n    def __init__(self, channels):\n        super(DownSampleBlock, self).__init__()\n        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n\n    def forward(self, x):\n        pad = (0, 1, 0, 1)\n        x = F.pad(x, pad, mode=\"constant\", value=0)\n        return self.conv(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Codebook(nn.Module):\n    \"\"\"\n    Codebook mapping: takes in an encoded image and maps each vector onto its closest codebook vector.\n    Metric: mean squared error = (z_e - z_q)**2 = (z_e**2) - (2*z_e*z_q) + (z_q**2)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.num_codebook_vectors = 256\n        self.latent_dim = 64\n        self.beta = 0.25\n\n        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n\n    def forward(self, z):\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.latent_dim)\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight ** 2, dim=1) - 2 * \\\n            torch.matmul(z_flattened, self.embedding.weight.t())\n\n        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n        z_q = self.embedding(min_encoding_indices).view(z.shape)\n\n        loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean((z_q - z.detach()) ** 2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()  # moving average instead of hard codebook remapping\n\n        z_q = z_q.permute(0, 3, 1, 2)\n\n        return z_q, min_encoding_indices, loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQVAE(nn.Module):\n    def __init__(self):\n        super(VQVAE,self).__init__()\n        channels=[64,128,128,256,256,512]\n        hid_dim=256\n        res=256\n        \n        layers_en=[nn.Conv2d(1,channels[0],3,1,1)]\n        for i in range(len(channels)-1):\n            ich=channels[i]\n            och=channels[i+1]\n            layers_en.append(nn.Conv2d(ich,och,3,1,1))\n            layers_en.append(nn.ReLU())\n            ich=och\n            if (res==16):\n                layers_en.append(NonLocalBlock(ich))\n            if i != len(channels) - 2:\n                layers_en.append(DownSampleBlock(channels[i+1]))\n                res //= 2\n        layers_en.append(nn.Sigmoid())\n        layers_en.append(nn.Conv2d(channels[-1], 64, 3, 1, 1))\n        block_in=channels[-1]\n        layers_de=[nn.Conv2d(64, block_in, kernel_size=3, stride=1, padding=1),\n                    NonLocalBlock(block_in)]\n\n        for i in reversed(range(len(channels)-1)):\n            block_out=channels[i]\n            layers_de.append(nn.Conv2d(block_in,block_out,3,1,1))\n            block_in=block_out\n            if i!=0:\n                layers_de.append(UpSampleBlock(block_in))\n                res*=2\n        layers_de.append(nn.Conv2d(block_in, 1, kernel_size=3, stride=1, padding=1))  \n            \n        self.encoder=nn.Sequential(*layers_en)  # 512,15,15\n        self.decoder=nn.Sequential(*layers_de)\n        self.codebook=Codebook()\n        self.quant_conv = nn.Conv2d(64, 64, 1)\n        self.post_quant_conv = nn.Conv2d(64, 64, 1)\n    def forward(self, imgs):\n        encoded_images = self.encoder(imgs)\n        quantized_encoded_images = self.quant_conv(encoded_images)\n        codebook_mapping, codebook_indices, q_loss = self.codebook(quantized_encoded_images)\n        quantized_codebook_mapping = self.post_quant_conv(codebook_mapping)\n        decoded_images = self.decoder(quantized_codebook_mapping)\n        return decoded_images, codebook_indices, q_loss\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=VQVAE().to(device)\nsummary(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#working\n# for i, data in enumerate(Batches,0):\n#         print(data.size())\n# inp= data.to(device)\nsample_input = torch.randn(45,1, 256,256).to(device)\n\n# Forward pass to obtain the output\ny = model(sample_input)\ngenerate_image = y[0][0].cpu().detach()\n\ngenerate_image_np = generate_image.numpy().squeeze()\n\ngenerate_image_np = (generate_image_np * 255).clip(0, 255) / 255.0\n# image_name = f'generate_img_epoch_{epoch + 1}.png'\n# image_path = os.path.join(CP_dir, image_name)\nplt.imshow(generate_image_np, cmap='gray')\nprint(y[0].shape)\n# break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 1e-3\nepochs=70\nbeta1=0.0\nbeta2=0.999\ncriterion=nn.MSELoss()\nopt_vq = torch.optim.Adam(model.parameters(),#list(model.encoder.parameters()) +\n#                           list(model.decoder.parameters()) +\n#                           list(model.codebook.parameters()) +\n#                           list(model.quant_conv.parameters()) +\n#                           list(model.post_quant_conv.parameters()),\n                          lr=lr, eps=1e-08, betas=(beta1,beta2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nfor epoch in range(epochs):\n    for i, data in enumerate(Batches,0):\n#         print(data.size())\n        inp= data.to(device)\n        inputs = inp\n        \n        # Train VAE\n        opt_vq.zero_grad()\n        reconstructions, _, q_loss = model(inputs)\n#         print(reconstructions, inputs)\n        \n        reconstruction_loss = criterion(reconstructions, inputs)\n        \n#         kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n        loss = reconstruction_loss +q_loss# kl_divergence\n        loss.backward()\n        opt_vq.step()\n\n        if (i + 1) % 50 == 0:  # Adjust the interval based on your needs\n            print(f'Epoch [{epoch + 2}/{epochs}], Batch [{i + 1}/{len(Batches)}], Loss: {loss.item()}')\n    with torch.no_grad():\n        model.eval()\n        x = torch.randn(1, 1, 256, 256).to(device)\n        y, _, _ = model(x)\n        generate_image = y[0][0].cpu().detach()\n\n    generate_image_np = generate_image.numpy().squeeze()\n\n    generate_image_np = (generate_image_np * 255).clip(0, 255) / 255.0\n    image_name = f'generate_img_epoch_{epoch + 1}.png'\n    image_path = os.path.join(CP_dir, image_name)\n    plt.imshow(generate_image_np, cmap='gray')\n    plt.imsave(image_path, generate_image_np, cmap='gray')\n\n    plt.title(f'Generated Image - Epoch {epoch + 1}')\n    plt.show()\n\n    # model checkpoints\n    checkpoint_vae = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': opt_vq.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint_vae, os.path.join(CP_dir, f'vae_checkpoint_epoch_{epoch + 1}.pth'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets\n\nmean = torch.tensor([0.491, 0.482, 0.447], device=device)\nstd = torch.tensor([0.247, 0.243, 0.262], device=device)\n\ndata_aug = transforms.Compose([\n    transforms.ToTensor(),\n#     transforms.Normalize(mean=mean, std=std),\n    transforms.Grayscale()])\ntrain_set = datasets.CIFAR10(root='.', \n                             train=True, \n                             download=True,\n                             transform=data_aug)\n\nBatches = DataLoader(train_set,\n                          batch_size=128, \n                          shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(Batches))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variance=0.06328692405746414","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i, (data,_) in enumerate(Batches,0):\n    print(data.size())\n    inp= data.to(device)\n    inputs = inp\n    fig,ax=plt.subplots(5,5,figsize=(14,14))\n    sample=[inputs[i] for i in range(25)]\n    images=load_image(filenames)\n    idx=0\n    print(sample[idx].cpu().shape)\n    for i in range(5):\n        for j in range(5):\n            ax[i,j].imshow(np.transpose(sample[idx].cpu(), (1,2,0)), cmap = 'gray')\n            idx+=1\n    # Train VAE\n    opt_vq.zero_grad()\n    reconstructions, _, q_loss = model(inputs)\n    print(reconstructions[0].shape)\n    fig,ax=plt.subplots(5,5,figsize=(14,14))\n    sample=[reconstructions[i] for i in range(25)]\n    images=load_image(filenames)\n    idx=0\n    for i in range(5):\n        for j in range(5):\n            ax[i,j].imshow(np.transpose(sample[idx].cpu().detach().numpy(), (1,2,0)), cmap = 'gray')\n            idx+=1\n    break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}